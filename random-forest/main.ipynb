{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树和随机森林\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取鸢尾花数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = pd.read_csv(\n",
    "    \"iris.data\",\n",
    "    names=[\n",
    "        \"sepal-length\",\n",
    "        \"sepal-width\",\n",
    "        \"petal-length\",\n",
    "        \"petal-width\",\n",
    "        \"species\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取收入数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_dataset = pd.read_csv(\n",
    "    \"adult.data\",\n",
    "    names=[\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"income\",\n",
    "    ],\n",
    "    nrows=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集划分为训练集和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(adult_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_gini` 函数计算一组计数的基尼系数。\n",
    "\n",
    "基尼系数代表了一个数据集的“混乱程度”。基尼系数越高，说明对应的数据集越混乱；反之则越整齐。\n",
    "\n",
    "例如，计数 `[3,3,4]` 的基尼系数为 1 - 0.3² - 0.3² - 0.4² = 0.66，说明这组计数背后的数据集很混乱。\n",
    "\n",
    "而计数 `[0,0,10]` 的基尼系数为 1 - 0² - 0² - 1² = 0，说明这组计数背后的数据集很整齐。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(counts: list[int]):\n",
    "    total = sum(counts)\n",
    "\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return 1 - sum((count / total) ** 2 for count in counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_weighted_gini` 函数计算若干组计数的加权平均基尼系数，其中每组计数的权重为：该组计数的总数 / 所有计数的总数。\n",
    "\n",
    "例如，`[3,3,4]` 和 `[0,0,10]` 这两组计数的加权平均基尼系数为 0.5 _ 0.66 + 0.5 _ 0 = 0.33。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_gini(counts_list: list[list[int]]):\n",
    "    total = sum(sum(counts) for counts in counts_list)\n",
    "    return sum((sum(counts) / total) * calculate_gini(counts) for counts in counts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`divide_by_discrete_feature` 函数基于指定的离散型特征，将数据集划分为两个子集。\n",
    "\n",
    "以收入数据集为例：基于“学历是否为本科”，可以将数据集划分为两个子集。\n",
    "\n",
    "在划分数据集时，该特征的所有可取的类别中，存在一个“最佳类别”。与基于其它“非最佳类别”的划分相比，基于“最佳类别”的划分可以使得两个子集的加权平均基尼系数最小。\n",
    "\n",
    "函数的第一个返回值是最佳类别，第二、三个返回值是划分后的两个子集，第四个返回值是加权平均基尼系数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_discrete_feature(dataset: pd.DataFrame, feature: str):\n",
    "    # 统计出所有可取的类别。\n",
    "    choices = dataset[feature].unique()\n",
    "\n",
    "    # 对于每个类别，都计算一遍划分后的加权平均基尼系数，然后找到使得加权平均基尼系数最小的最佳类别。\n",
    "    best_choice = None\n",
    "    best_yes_subset = None\n",
    "    best_no_subset = None\n",
    "    min_gini = 1.0\n",
    "    for choice in choices:\n",
    "        # 根据“特征是否取该类别”划分出两个子集。\n",
    "        yes_subset: pd.DataFrame = dataset[dataset[feature] == choice]\n",
    "        no_subset: pd.DataFrame = dataset[dataset[feature] != choice]\n",
    "\n",
    "        # 分别统计两个子集中，各标签的数量。\n",
    "        yes_counts = yes_subset.iloc[:, -1].value_counts()\n",
    "        no_counts = no_subset.iloc[:, -1].value_counts()\n",
    "\n",
    "        # 计算当前划分下的加权平均基尼系数。\n",
    "        gini = calculate_weighted_gini([yes_counts, no_counts])\n",
    "\n",
    "        # 更新最佳类别和最小系数。\n",
    "        if gini < min_gini:\n",
    "            best_choice = choice\n",
    "            best_yes_subset = yes_subset\n",
    "            best_no_subset = no_subset\n",
    "            min_gini = gini\n",
    "\n",
    "    return best_choice, best_yes_subset, best_no_subset, min_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`divide_by_continuous_feature` 函数基于指定的连续型特征，将数据集划分为两个子集。\n",
    "\n",
    "以收入数据集为例：基于“年龄是否小于 30.5 岁”，可以将数据集划分为两个子集。\n",
    "\n",
    "为了将连续的特征离散化，我们取该特征下所有相邻值的二分点，作为若干个可能的阈值。\n",
    "\n",
    "在划分数据集时，该特征的所有可能的阈值中，存在一个“最佳阈值”。与基于其它“非最佳阈值”的划分相比，基于“最佳阈值”的划分可以使得两个子集的加权平均基尼系数最小。\n",
    "\n",
    "函数的第一个返回值是最佳阈值，第二、三个返回值是划分后的两个子集，第四个返回值是加权平均基尼系数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_continuous_feature(dataset: pd.DataFrame, feature: str):\n",
    "    # 计算出所有可能的阈值。\n",
    "    unique_values = dataset[feature].unique()\n",
    "    thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "\n",
    "    # 对于每个阈值，都计算一遍划分后的加权平均基尼系数，然后找到使得加权平均基尼系数最小的最佳阈值。\n",
    "    best_threshold = None\n",
    "    best_less_subset = None\n",
    "    best_greater_subset = None\n",
    "    min_gini = 1.0\n",
    "    for threshold in thresholds:\n",
    "        # 根据“特征是否小于该阈值”划分出两个子集。\n",
    "        less_subset: pd.DataFrame = dataset[dataset[feature] < threshold]\n",
    "        greater_subset: pd.DataFrame = dataset[dataset[feature] > threshold]\n",
    "\n",
    "        # 分别统计两个子集中，各标签的数量。\n",
    "        less_counts = less_subset.iloc[:, -1].value_counts()\n",
    "        greater_counts = greater_subset.iloc[:, -1].value_counts()\n",
    "\n",
    "        # 计算当前划分下的加权平均基尼系数。\n",
    "        gini = calculate_weighted_gini([less_counts, greater_counts])\n",
    "\n",
    "        # 更新最佳阈值和最小系数。\n",
    "        if gini < min_gini:\n",
    "            best_threshold = threshold\n",
    "            best_less_subset = less_subset\n",
    "            best_greater_subset = greater_subset\n",
    "            min_gini = gini\n",
    "\n",
    "    return best_threshold, best_less_subset, best_greater_subset, min_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_best_division` 函数找到数据集的最佳划分。\n",
    "\n",
    "在划分数据集时，它的所有特征中，存在一个“最佳特征”。与基于其它“非最佳特征”的划分相比，基于“最佳特征”的划分可以使得两个子集的加权平均基尼系数最小。\n",
    "\n",
    "函数的第一个返回值是最佳特征，第二个返回值是最佳类别（若非离散型特征则为 `None`），第三个返回值是最佳阈值（若非连续型特征则为 `None`），第四、五个返回值是划分后的两个子集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_division(dataset: pd.DataFrame):\n",
    "    # 统计出所有可能的特征。\n",
    "    features = dataset.columns[:-1]\n",
    "\n",
    "    # 对于每个特征，都计算一遍划分后的加权平均基尼系数，然后找到使得加权平均基尼系数最小的最佳特征。\n",
    "    best_feature = None\n",
    "    best_choice = None\n",
    "    best_threshold = None\n",
    "    best_left_subset = None\n",
    "    best_right_subset = None\n",
    "    min_gini = 1.0\n",
    "    for feature in features:\n",
    "        # 在标签相同的情况下，如果该特征下只有一个类别或取值，\n",
    "        # 说明该特征无法继续划分，直接跳过。\n",
    "        if len(dataset[feature].unique()) == 1:\n",
    "            continue\n",
    "\n",
    "        # 如果是离散型特征，则找到最佳类别。\n",
    "        if dataset[feature].dtype == object:\n",
    "            choice, left_subset, right_subset, gini = divide_by_discrete_feature(\n",
    "                dataset, feature\n",
    "            )\n",
    "            threshold = None\n",
    "\n",
    "        # 如果是连续型特征，则找到最佳阈值。\n",
    "        else:\n",
    "            (\n",
    "                threshold,\n",
    "                left_subset,\n",
    "                right_subset,\n",
    "                gini,\n",
    "            ) = divide_by_continuous_feature(dataset, feature)\n",
    "            choice = None\n",
    "\n",
    "        # 更新最佳特征、最佳类别、最佳阈值和最小系数。\n",
    "        if gini < min_gini:\n",
    "            best_feature = feature\n",
    "            best_choice = choice\n",
    "            best_threshold = threshold\n",
    "            best_left_subset = left_subset\n",
    "            best_right_subset = right_subset\n",
    "            min_gini = gini\n",
    "\n",
    "    return (\n",
    "        best_feature,\n",
    "        best_choice,\n",
    "        best_threshold,\n",
    "        best_left_subset,\n",
    "        best_right_subset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_decision_tree` 递归构建决策树。\n",
    "\n",
    "决策树每个节点的数据结构如下：\n",
    "\n",
    "- `feature`：划分数据集的特征。\n",
    "- `choice`：如果 `feature` 是离散型的，那么 `choice` 是最佳类别；否则为 `None`。\n",
    "- `threshold`：如果 `feature` 是连续型的，那么 `threshold` 是最佳阈值；否则为 `None`。\n",
    "- `left`：该节点的左子树。\n",
    "- `right`：该节点的右子树。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(dataset: pd.DataFrame, current_depth: int = 1):\n",
    "    # 如果数据集中只有一种标签，那么直接返回这个类别。\n",
    "    if len(dataset.iloc[:, -1].unique()) == 1:\n",
    "        return dataset.iloc[:, -1].unique()[0]\n",
    "\n",
    "    # 找到该数据集的最佳特征与最佳类别（或阈值）。\n",
    "    feature, choice, threshold, left_subset, right_subset = find_best_division(dataset)\n",
    "\n",
    "    # 如果找不到最佳特征，说明所有特征都只有一种类别或取值。\n",
    "    # 用投票法决定该节点的类别。\n",
    "    if feature is None:\n",
    "        return dataset.iloc[:, -1].value_counts().index[0]\n",
    "\n",
    "    # 递归地构建左右子树。\n",
    "    left_tree = build_decision_tree(left_subset, current_depth + 1)\n",
    "    right_tree = build_decision_tree(right_subset, current_depth + 1)\n",
    "\n",
    "    # 返回当前节点。\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"choice\": choice,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left_tree,\n",
    "        \"right\": right_tree,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict` 函数预测一行数据的标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row: pd.Series, decision_tree: dict):\n",
    "    # 如果当前节点是叶子节点，那么直接返回这个节点的标签。\n",
    "    if isinstance(decision_tree, str):\n",
    "        return decision_tree\n",
    "\n",
    "    # 如果当前节点的特征是离散型，则根据其指示的类别，判断是进入左子树还是右子树。\n",
    "    if decision_tree[\"choice\"] != None:\n",
    "        if row[decision_tree[\"feature\"]] == decision_tree[\"choice\"]:\n",
    "            return predict(row, decision_tree[\"left\"])\n",
    "        else:\n",
    "            return predict(row, decision_tree[\"right\"])\n",
    "\n",
    "    # 如果当前节点的特征是连续型，则根据其指示的阈值，判断是进入左子树还是右子树。\n",
    "    elif decision_tree[\"threshold\"] != None:\n",
    "        if row[decision_tree[\"feature\"]] < decision_tree[\"threshold\"]:\n",
    "            return predict(row, decision_tree[\"left\"])\n",
    "        else:\n",
    "            return predict(row, decision_tree[\"right\"])\n",
    "\n",
    "    # 如果当前节点的特征既不是离散型，也不是连续型，则抛出异常。\n",
    "    else:\n",
    "        raise Exception(\"Invalid decision tree node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test` 函数在指定的数据集上，测试决策树的准确率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset: pd.DataFrame, decision_tree: dict):\n",
    "    predictions = dataset.apply(predict, axis=1, args=(decision_tree,))\n",
    "    accuracy = sum(predictions == dataset.iloc[:, -1]) / dataset.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "decision_tree = build_decision_tree(train_set)\n",
    "accuracy = test(test_set, decision_tree)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample_dataset` 函数使用 Bootstrap 算法，从数据集中有放回地抽取同样多行数据，并随机选取一小部分属性，构成一个新的数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset: pd.DataFrame, num_samples: int = 30):\n",
    "    num_features = int(np.sqrt(dataset.shape[1]))\n",
    "    samples: list[pd.DataFrame] = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sample = dataset.sample(dataset.shape[0], replace=True)\n",
    "        features = np.random.choice(sample.columns[:-1], num_features, replace=False)\n",
    "        sample = pd.concat([sample[features], sample.iloc[:, -1]], axis=1)\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_random_forest` 函数在指定的数据集上，构建随机森林。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_forest(dataset: pd.DataFrame):\n",
    "    samples = sample_dataset(dataset)\n",
    "    decision_trees = [build_decision_tree(sample) for sample in samples]\n",
    "    return decision_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_with_random_forest` 函数使用随机森林预测一行数据的标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_random_forest(row: pd.Series, random_forest: list[dict]):\n",
    "    predictions = [predict(row, decision_tree) for decision_tree in random_forest]\n",
    "    return max(set(predictions), key=predictions.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_with_random_forest` 函数在指定的数据集上，测试随机森林的准确率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_random_forest(dataset: pd.DataFrame, random_forest: list[dict]):\n",
    "    predictions = dataset.apply(\n",
    "        predict_with_random_forest, axis=1, args=(random_forest,)\n",
    "    )\n",
    "    accuracy = sum(predictions == dataset.iloc[:, -1]) / dataset.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "random_forest = build_random_forest(train_set)\n",
    "accuracy = test_with_random_forest(test_set, random_forest)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comprehensive-curriculum-design-of-data-an-ZStDsHtf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
