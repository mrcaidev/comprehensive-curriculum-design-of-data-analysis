{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 节点重要性排序和评估\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个网络是一张无权无向图，使用它的邻接矩阵来表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = np.ndarray[np.ndarray[0 | 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载若干个网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"networks.json\", \"r\") as file:\n",
    "    networks: list[Network] = [np.asarray(network) for network in json.load(file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_neighbors` 函数找到一个节点在网络中的邻居。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(network: Network, node: int):\n",
    "    return np.nonzero(network[node])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simulate_SIR` 函数模拟 SIR 传播过程。\n",
    "\n",
    "SIR 传播过程如下：\n",
    "\n",
    "1. 在一个网络中，存在一些源感染者。为了简单起见，不妨假设源感染者只有一位。\n",
    "2. 在每一轮传播中，每个感染者以概率 β 感染其邻居，以概率 γ 康复。\n",
    "3. 当网络中的所有节点都康复时，传播过程结束。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_SIR(\n",
    "    network: Network, source: int = 0, beta: float = 0.3, gamma: float = 0.1\n",
    "):\n",
    "    # 初始化各节点的状态。\n",
    "    susceptible_nodes = set(range(network.shape[0])) - {source}\n",
    "    infected_nodes = {source}\n",
    "    recovered_nodes = set()\n",
    "\n",
    "    # 记录各状态在各时刻上的节点数量。\n",
    "    susceptible_counts = [len(susceptible_nodes)]\n",
    "    infected_counts = [len(infected_nodes)]\n",
    "    recovered_counts = [len(recovered_nodes)]\n",
    "\n",
    "    # 如果还有感染者，SIR 传播过程就会继续。\n",
    "    while infected_nodes:\n",
    "        # 记录本轮新感染和新康复的节点。\n",
    "        new_infected_nodes = set()\n",
    "        new_recovered_nodes = set()\n",
    "\n",
    "        # 对于每个感染者来说：\n",
    "        for infected_node in infected_nodes:\n",
    "            # 他会以概率 β 感染其邻居。\n",
    "            neighbors = find_neighbors(network, infected_node)\n",
    "            susceptible_neighbors = set(neighbors) & susceptible_nodes\n",
    "            new_infected_neighbors = {\n",
    "                susceptible_neighbor\n",
    "                for susceptible_neighbor in susceptible_neighbors\n",
    "                if np.random.rand() < beta\n",
    "            }\n",
    "            new_infected_nodes |= new_infected_neighbors\n",
    "\n",
    "            # 他会以概率 γ 康复。\n",
    "            if np.random.rand() < gamma:\n",
    "                new_recovered_nodes.add(infected_node)\n",
    "\n",
    "        # 更新各节点状态。\n",
    "        susceptible_nodes -= new_infected_nodes\n",
    "        infected_nodes = (infected_nodes | new_infected_nodes) - new_recovered_nodes\n",
    "        recovered_nodes |= new_recovered_nodes\n",
    "\n",
    "        # 更新各状态在当前时刻上的节点数量。\n",
    "        susceptible_counts.append(len(susceptible_nodes))\n",
    "        infected_counts.append(len(infected_nodes))\n",
    "        recovered_counts.append(len(recovered_nodes))\n",
    "\n",
    "    # 打包计数结果。\n",
    "    counts = {\n",
    "        \"susceptible\": susceptible_counts,\n",
    "        \"infected\": infected_counts,\n",
    "        \"recovered\": recovered_counts,\n",
    "    }\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`visualize_SIR` 函数绘制 SIR 传播过程中，各状态在各时刻上的节点数量的堆积面积图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_SIR(counts: dict[str, list[int]]):\n",
    "    plt.stackplot(\n",
    "        range(len(counts[\"infected\"])),\n",
    "        counts.values(),\n",
    "        labels=counts.keys(),\n",
    "        colors=[\"blue\", \"red\", \"green\"],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.title(\"SIR Propagation\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Number\")\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿真一次 SIR 传播过程，并可视化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = simulate_SIR(networks[0])\n",
    "visualize_SIR(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_correlation` 函数计算两个序列的相关性系数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(sequence1: list[int], sequence2: list[int]):\n",
    "    # 计算两个长度为 1 的序列的相关性系数没有意义。\n",
    "    assert len(sequence1) > 1 and len(sequence2) > 1\n",
    "\n",
    "    # 逐一比较每一个变化对的正相关性和负相关性。\n",
    "    pairs = list(zip(sequence1, sequence2))\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "\n",
    "    for index, (x1, y1) in enumerate(pairs[:-1]):\n",
    "        for x2, y2 in pairs[index + 1 :]:\n",
    "            if (x1 < x2 and y1 < y2) or (x1 > x2 and y1 > y2):\n",
    "                num_positive += 1\n",
    "            elif (x1 < x2 and y1 > y2) or (x1 > x2 and y1 < y2):\n",
    "                num_negative += 1\n",
    "\n",
    "    # 导出相关性系数。\n",
    "    total = np.sum(range(len(sequence1)))\n",
    "    correlation = np.round((num_positive - num_negative) / total, 3)\n",
    "\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`k_shell_sort` 函数使用 k-shell 分解法，对节点进行排序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shell_sort(network: Network):\n",
    "    # 转换为浮点型，以便在后面用 `inf` 表示已移除的节点。\n",
    "    network = network.astype(float)\n",
    "\n",
    "    # 初始化每个节点的度数。\n",
    "    degrees = np.asarray([np.sum(row) for row in network])\n",
    "\n",
    "    # 存储排序后的节点。\n",
    "    order = []\n",
    "\n",
    "    # 如果还有节点没被排序，排序过程就会继续。\n",
    "    while len(order) < network.shape[0]:\n",
    "        # 找到度数最小的节点。\n",
    "        min_degree = np.min(degrees)\n",
    "        min_degree_nodes = np.nonzero(degrees == min_degree)[0]\n",
    "\n",
    "        # 将这些节点从网络中移除。\n",
    "        network[:, min_degree_nodes] = 0\n",
    "        network[min_degree_nodes, :] = np.inf\n",
    "\n",
    "        # 更新每个节点的度数。\n",
    "        degrees = np.asarray([np.sum(row) for row in network])\n",
    "\n",
    "        # 将这些节点添加到排序后的节点中。\n",
    "        order.extend((node, int(min_degree)) for node in min_degree_nodes)\n",
    "\n",
    "    return order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`group_nodes_by_distance` 函数将节点按照距离分组。\n",
    "\n",
    "例如，分组结果 `[[0],[1,2],[3,4]]` 代表：节点 0 和节点 0 的距离为 0，节点 1、2 和节点 0 的距离为 1，节点 3、4 和节点 0 的距离为 2。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_nodes_by_distance(network: Network, source: int):\n",
    "    # 记录各层内的节点。\n",
    "    layers = defaultdict(list)\n",
    "\n",
    "    # 从源节点开始 BFS。\n",
    "    queue = [(source, 0)]\n",
    "    visited_nodes = {source}\n",
    "\n",
    "    while queue:\n",
    "        # 弹出队头节点，并记录其和源节点的距离。\n",
    "        node, distance = queue.pop(0)\n",
    "        layers[distance].append(node)\n",
    "\n",
    "        # 访问该节点的未被访问的邻居，并将它们加入待记录的队列。\n",
    "        neighbors = find_neighbors(network, node)\n",
    "        unvisited_neighbors = set(neighbors) - visited_nodes\n",
    "        queue.extend((neighbor, distance + 1) for neighbor in unvisited_neighbors)\n",
    "        visited_nodes |= unvisited_neighbors\n",
    "\n",
    "    # 根据字典键大小顺序，转换为嵌套列表。\n",
    "    layers = [layers[distance] for distance in sorted(layers.keys())]\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_probabilistic_importance` 函数计算概率模型下，一个节点的重要性分数，也即各节点被其感染的感染分数之和。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilistic_importance(network: Network, source: int, beta: float):\n",
    "    # 记录各节点的感染分数。\n",
    "    scores = np.zeros(network.shape[0])\n",
    "    scores[source] = 1\n",
    "\n",
    "    # 按照到源节点的距离分层。\n",
    "    layers = group_nodes_by_distance(network, source)\n",
    "\n",
    "    # 在 1-3 层上，从近到远，计算每个节点的感染分数。\n",
    "    for layer_index, layer in enumerate(layers[1:4]):\n",
    "        for node in layer:\n",
    "            # 找到该节点处于前一层的邻居。\n",
    "            neighbors = find_neighbors(network, node)\n",
    "            influencers = np.intersect1d(neighbors, layers[layer_index])\n",
    "\n",
    "            # 根据这些邻居的感染分数，计算该节点的感染分数。\n",
    "            scores[node] = 1 - np.prod(1 - scores[influencers] * beta)\n",
    "\n",
    "    # 导出重要性分数。\n",
    "    importance = np.round(np.sum(scores) - 1, 3)\n",
    "\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`probabilistic_sort` 函数使用概率模型，对节点进行排序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_sort(network: Network, beta: float = 0.3):\n",
    "    with_importance_nodes = [\n",
    "        (node, calculate_probabilistic_importance(network, node, beta))\n",
    "        for node in range(network.shape[0])\n",
    "    ]\n",
    "    order = sorted(with_importance_nodes, key=lambda pair: pair[1])\n",
    "    return order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SIR_sort` 函数使用 SIR 模型，对节点进行排序。\n",
    "\n",
    "每个节点的分数是：以它为感染源时，最终康复的节点数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_sort(network: Network, beta: float = 0.3, gamma=0.1):\n",
    "    with_importance_nodes = [\n",
    "        (node, simulate_SIR(network, node, beta, gamma)[\"recovered\"][-1])\n",
    "        for node in range(network.shape[0])\n",
    "    ]\n",
    "    order = sorted(with_importance_nodes, key=lambda pair: pair[1])\n",
    "    return order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_score_dataframe` 函数使用 Dataframe 来展示节点的分数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_dataframe(\n",
    "    SIR_sequence: list[tuple[int, int]],\n",
    "    k_shell_sequence: list[tuple[int, int]],\n",
    "    probabilistic_sequence: list[tuple[int, float]],\n",
    "):\n",
    "    dataframe = pd.DataFrame(\n",
    "        columns=[\"SIR\", \"K-shell\", \"Probabilistic\"],\n",
    "        index=range(len(SIR_sequence)),\n",
    "    )\n",
    "\n",
    "    for node, score in SIR_sequence:\n",
    "        dataframe.loc[node, \"SIR\"] = score\n",
    "\n",
    "    for node, score in k_shell_sequence:\n",
    "        dataframe.loc[node, \"K-shell\"] = score\n",
    "\n",
    "    for node, score in probabilistic_sequence:\n",
    "        dataframe.loc[node, \"Probabilistic\"] = score\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估几种排序结果和 SIR 仿真结果的相关性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_index, network in enumerate(networks):\n",
    "    SIR_sequence = SIR_sort(network, beta=0.2)\n",
    "    k_shell_sequence = k_shell_sort(network)\n",
    "    probabilistic_sequence = probabilistic_sort(network, beta=0.2)\n",
    "\n",
    "    score_dataframe = create_score_dataframe(\n",
    "        SIR_sequence, k_shell_sequence, probabilistic_sequence\n",
    "    )\n",
    "\n",
    "    SK_correlation = calculate_correlation(\n",
    "        [pair[0] for pair in SIR_sequence],\n",
    "        [pair[0] for pair in k_shell_sequence],\n",
    "    )\n",
    "    SP_correlation = calculate_correlation(\n",
    "        [pair[0] for pair in SIR_sequence],\n",
    "        [pair[0] for pair in probabilistic_sequence],\n",
    "    )\n",
    "\n",
    "    print(f\"Network #{network_index + 1}:\", end=\"\\n\\n\")\n",
    "    print(score_dataframe, end=\"\\n\\n\")\n",
    "    print(f\"SIR ~ K-shell correlation: {SK_correlation}\")\n",
    "    print(f\"SIR ~ Probabilistic correlation: {SP_correlation}\", end=\"\\n\\n\")\n",
    "    print(\"-----------------------------------------\", end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comprehensive-curriculum-design-of-data-an-ZStDsHtf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
